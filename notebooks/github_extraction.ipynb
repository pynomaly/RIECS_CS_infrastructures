{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffc11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando repositorios de citizen science...\n",
      "Obtenidos 100 repositorios...\n",
      "Obtenidos 200 repositorios...\n",
      "Obtenidos 300 repositorios...\n",
      "Obtenidos 400 repositorios...\n",
      "Obtenidos 500 repositorios...\n",
      "Obtenidos 600 repositorios...\n",
      "Obtenidos 700 repositorios...\n",
      "Obtenidos 800 repositorios...\n",
      "Obtenidos 900 repositorios...\n",
      "Obtenidos 918 repositorios...\n",
      "Obtenidos 1000 repositorios...\n",
      "Encontrados 1000 repositorios\n",
      "Datos guardados en github_repositories_20250616_155622.csv\n",
      "\n",
      "=== ESTAD√çSTICAS ===\n",
      "Total de repositorios: 1000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class GitHubRepoExtractor:\n",
    "    def __init__(self, token=None):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository extractor\n",
    "        \"\"\"\n",
    "        self.token = token\n",
    "        self.base_url = \"https://api.github.com\"\n",
    "        self.headers = {\n",
    "            'Accept': 'application/vnd.github.v3+json',\n",
    "            'User-Agent': 'Python-GitHub-Extractor'\n",
    "        }\n",
    "        \n",
    "        if token:\n",
    "            self.headers['Authorization'] = f'token {token}'\n",
    "    \n",
    "    def search_repositories(self, query, sort='stars', order='desc', per_page=100, max_results=1000):\n",
    "\n",
    "        repositories = []\n",
    "        page = 1\n",
    "        \n",
    "        while len(repositories) < max_results:\n",
    "            url = f\"{self.base_url}/search/repositories\"\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'sort': sort,\n",
    "                'order': order,\n",
    "                'per_page': min(per_page, max_results - len(repositories)),\n",
    "                'page': page\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, params=params)\n",
    "                \n",
    "                if response.status_code == 403:\n",
    "                    print(\"Rate limit reached. Waiting...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                \n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                if not data['items']:\n",
    "                    break\n",
    "                \n",
    "                repositories.extend(data['items'])\n",
    "                \n",
    "                print(f\"Obtained {len(repositories)} repositories...\")\n",
    "                \n",
    "                if not self.token:\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                page += 1\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                break\n",
    "        \n",
    "        return repositories[:max_results]\n",
    "    \n",
    "\n",
    "    def extract_repo_data(self, repo):\n",
    "        \"\"\"\n",
    "        Extract relevant data from a repository\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'name': repo.get('name'),\n",
    "            'full_name': repo.get('full_name'),\n",
    "            'description': repo.get('description'),\n",
    "            'url': repo.get('html_url'),\n",
    "            'language': repo.get('language'),\n",
    "            'stars': repo.get('stargazers_count', 0),\n",
    "            'forks': repo.get('forks_count', 0),\n",
    "            'watchers': repo.get('watchers_count', 0),\n",
    "            'size': repo.get('size', 0),\n",
    "            'created_at': repo.get('created_at'),\n",
    "            'updated_at': repo.get('updated_at'),\n",
    "            'topics': repo.get('topics', []),\n",
    "            'license': repo.get('license', {}).get('name') if repo.get('license') else None,\n",
    "            'owner': repo.get('owner', {}).get('login'),\n",
    "            'owner_type': repo.get('owner', {}).get('type'),\n",
    "            'is_fork': repo.get('fork', False),\n",
    "            'has_issues': repo.get('has_issues', False),\n",
    "            'has_wiki': repo.get('has_wiki', False),\n",
    "            'has_pages': repo.get('has_pages', False),\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def save_to_csv(self, repositories, filename=None):\n",
    "        \"\"\"\n",
    "        Save repositories to a CSV file\n",
    "        \"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"../data/github_repositories_{timestamp}.csv\"\n",
    "        \n",
    "        processed_repos = [self.extract_repo_data(repo) for repo in repositories]\n",
    "        \n",
    "        df = pd.DataFrame(processed_repos)\n",
    "        \n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Data saved in {filename}\")\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca026ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize extractor\n",
    "extractor = GitHubRepoExtractor()  # token=\"tu_token_aqui\"\n",
    "    \n",
    "# Search repositories related to \"citizen science\"\n",
    "print(\"Buscando repositorios de citizen science...\")\n",
    "repositories = extractor.search_repositories(\n",
    "    query=\"citizen science\",\n",
    "    sort=\"stars\",\n",
    "    order=\"desc\",\n",
    "    max_results=1000\n",
    ")\n",
    "\n",
    "print(f\"Encontrados {len(repositories)} repositorios\")\n",
    "\n",
    "# Save to CSV\n",
    "df = extractor.save_to_csv(repositories)\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"Total number of repositories: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a30a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates and save final csv\n",
    "\n",
    "df = pd.read_csv(\"github_repositories_20250616_155622.csv\")\n",
    "\n",
    "df.drop_duplicates().to_csv(\"github_repositories.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
